{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQUk3Y0WwYZ4"
      },
      "source": [
        "# 🤗 x 🦾: Training SmolVLA with LeRobot Notebook\n",
        "\n",
        "Welcome to the **LeRobot SmolVLA training notebook**! This notebook provides a ready-to-run setup for training imitation learning policies using the [🤗 LeRobot](https://github.com/huggingface/lerobot) library.\n",
        "\n",
        "In this example, we train an `SmolVLA` policy using a dataset hosted on the [Hugging Face Hub](https://huggingface.co/), and optionally track training metrics with [Weights & Biases (wandb)](https://wandb.ai/).\n",
        "\n",
        "## ⚙️ Requirements\n",
        "- A Hugging Face dataset repo ID containing your training data (`--dataset.repo_id=YOUR_USERNAME/YOUR_DATASET`)\n",
        "- Optional: A [wandb](https://wandb.ai/) account if you want to enable training visualization\n",
        "- Recommended: GPU runtime (e.g., NVIDIA A100) for faster training\n",
        "\n",
        "## ⏱️ Expected Training Time\n",
        "Training with the `SmolVLA` policy for 20,000 steps typically takes **about 5 hours on an NVIDIA A100** GPU. On less powerful GPUs or CPUs, training may take significantly longer!\n",
        "\n",
        "## Example Output\n",
        "Model checkpoints, logs, and training plots will be saved to the specified `--output_dir`. If `wandb` is enabled, progress will also be visualized in your wandb project dashboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PolVM_movEvp",
        "outputId": "c1739729-7bf5-4f4d-e4a4-51b2f3f20e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdamin3927\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yu5khQGIHi6",
        "outputId": "6d447f58-eaca-43f5-cbd8-057878cfb5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "The token `colab smolvla ft` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `colab smolvla ft`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlKjL1X5t_zM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7848a90-839d-49b1-b830-73a57bb72325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgLu7QT5tUik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56f4569-250c-4483-c4f4-594049be0857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lerobot' already exists and is not an empty directory.\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Solving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.3\n",
            "    latest version: 25.5.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Obtaining file:///content/lerobot\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: av>=14.2.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (14.4.0)\n",
            "Requirement already satisfied: cmake>=3.29.0.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: datasets>=2.19.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: deepdiff>=7.0.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (8.5.0)\n",
            "Requirement already satisfied: diffusers>=0.27.2 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.34.0)\n",
            "Requirement already satisfied: draccus==0.10.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.8.1)\n",
            "Requirement already satisfied: flask>=3.0.3 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.1.1)\n",
            "Requirement already satisfied: gdown>=5.1.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (5.2.0)\n",
            "Requirement already satisfied: gymnasium==0.29.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.29.1)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.1 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.33.1)\n",
            "Requirement already satisfied: imageio>=2.34.0 in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]>=2.34.0->lerobot==0.1.0) (2.37.0)\n",
            "Requirement already satisfied: jsonlines>=4.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (4.0.0)\n",
            "Requirement already satisfied: numba>=0.59.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.61.2)\n",
            "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (4.11.0.86)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (24.2)\n",
            "Requirement already satisfied: pymunk<7.0.0,>=6.6.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (6.11.1)\n",
            "Requirement already satisfied: pynput>=1.7.7 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (1.8.1)\n",
            "Requirement already satisfied: pyserial>=3.5 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.5)\n",
            "Requirement already satisfied: pyzmq>=26.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (27.0.0)\n",
            "Requirement already satisfied: rerun-sdk>=0.21.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.14.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=2.4.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: torch>=2.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (2.7.1)\n",
            "Requirement already satisfied: torchcodec>=0.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.4.0)\n",
            "Requirement already satisfied: torchvision>=0.21.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.22.1)\n",
            "Requirement already satisfied: wandb>=0.16.3 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.20.1)\n",
            "Requirement already satisfied: zarr>=2.17.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.0.8)\n",
            "Requirement already satisfied: mergedeep~=1.3 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (1.3.4)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: pyyaml-include~=1.4 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: toml~=0.10 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (0.10.2)\n",
            "Requirement already satisfied: typing-inspect~=0.9.0 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/site-packages (from gymnasium==0.29.1->lerobot==0.1.0) (2.2.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/site-packages (from gymnasium==0.29.1->lerobot==0.1.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/site-packages (from gymnasium==0.29.1->lerobot==0.1.0) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/site-packages (from gymnasium==0.29.1->lerobot==0.1.0) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: orderly-set<6,>=5.4.1 in /usr/local/lib/python3.11/site-packages (from deepdiff>=7.0.1->lerobot==0.1.0) (5.4.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/site-packages (from diffusers>=0.27.2->lerobot==0.1.0) (8.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from diffusers>=0.27.2->lerobot==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/site-packages (from diffusers>=0.27.2->lerobot==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/site-packages (from diffusers>=0.27.2->lerobot==0.1.0) (11.2.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (3.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/site-packages (from gdown>=5.1.0->lerobot==0.1.0) (4.13.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.27.1->huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (1.1.5)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.3.4)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.1.9)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (3.0.51)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]>=2.34.0->lerobot==0.1.0) (0.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]>=2.34.0->lerobot==0.1.0) (7.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/site-packages (from jsonlines>=4.0.0->lerobot==0.1.0) (25.3.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/site-packages (from numba>=0.59.0->lerobot==0.1.0) (0.44.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/site-packages (from omegaconf>=2.3.0->lerobot==0.1.0) (4.9.3)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/site-packages (from pymunk<7.0.0,>=6.6.0->lerobot==0.1.0) (1.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from pynput>=1.7.7->lerobot==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: evdev>=1.3 in /usr/local/lib/python3.11/site-packages (from pynput>=1.7.7->lerobot==0.1.0) (1.9.2)\n",
            "Requirement already satisfied: python-xlib>=0.17 in /usr/local/lib/python3.11/site-packages (from pynput>=1.7.7->lerobot==0.1.0) (0.33)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.3.1->torch>=2.2.1->lerobot==0.1.0) (65.6.3)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (6.31.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (2.32.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (1.3.6)\n",
            "Requirement already satisfied: donfig>=0.8 in /usr/local/lib/python3.11/site-packages (from zarr>=2.17.0->lerobot==0.1.0) (0.8.1.post1)\n",
            "Requirement already satisfied: numcodecs>=0.14 in /usr/local/lib/python3.11/site-packages (from numcodecs[crc32c]>=0.14->zarr>=2.17.0->lerobot==0.1.0) (0.16.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.17.1->pymunk<7.0.0,>=6.6.0->lerobot==0.1.0) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (3.12.13)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.16.3->lerobot==0.1.0) (4.0.12)\n",
            "Requirement already satisfied: crc32c>=2.7 in /usr/local/lib/python3.11/site-packages (from numcodecs[crc32c]>=0.14->zarr>=2.17.0->lerobot==0.1.0) (2.7.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb>=0.16.3->lerobot==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb>=0.16.3->lerobot==0.1.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb>=0.16.3->lerobot==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot==0.1.0) (2025.6.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.2.1->lerobot==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4->gdown>=5.1.0->lerobot==0.1.0) (2.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/site-packages (from importlib_metadata->diffusers>=0.27.2->lerobot==0.1.0) (3.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->lerobot==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->lerobot==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->lerobot==0.1.0) (2025.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/site-packages (from requests[socks]->gdown>=5.1.0->lerobot==0.1.0) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (6.6.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.16.3->lerobot==0.1.0) (5.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.2.13)\n",
            "Building wheels for collected packages: lerobot\n",
            "  Building editable for lerobot (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lerobot: filename=lerobot-0.1.0-py3-none-any.whl size=16366 sha256=15dcbc92fc8fe9542afd3f36129de5af8ab5b343281d499ad864d1e0e716556f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-amjkst7m/wheels/15/0d/02/b9c6ff1c78574dee99101ad231194b3425eb4cd784ce8c8338\n",
            "Successfully built lerobot\n",
            "Installing collected packages: lerobot\n",
            "  Attempting uninstall: lerobot\n",
            "    Found existing installation: lerobot 0.1.0\n",
            "    Uninstalling lerobot-0.1.0:\n",
            "      Successfully uninstalled lerobot-0.1.0\n",
            "Successfully installed lerobot-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "!conda install ffmpeg=7.1.1 -c conda-forge\n",
        "!cd lerobot && pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwAd1SQGA81j"
      },
      "source": [
        "## Install SmolVLA dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ2kwiojA81j",
        "outputId": "3c61b2fc-ea33-407a-c81b-7d89a9c48f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/lerobot\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: av>=14.2.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (14.4.0)\n",
            "Requirement already satisfied: cmake>=3.29.0.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: datasets>=2.19.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: deepdiff>=7.0.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (8.5.0)\n",
            "Requirement already satisfied: diffusers>=0.27.2 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.34.0)\n",
            "Requirement already satisfied: draccus==0.10.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.8.1)\n",
            "Requirement already satisfied: flask>=3.0.3 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.1.1)\n",
            "Requirement already satisfied: gdown>=5.1.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (5.2.0)\n",
            "Requirement already satisfied: gymnasium==0.29.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.29.1)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.1 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.33.1)\n",
            "Requirement already satisfied: imageio>=2.34.0 in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]>=2.34.0->lerobot==0.1.0) (2.37.0)\n",
            "Requirement already satisfied: jsonlines>=4.0.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (4.0.0)\n",
            "Requirement already satisfied: numba>=0.59.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.61.2)\n",
            "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (4.11.0.86)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (24.2)\n",
            "Requirement already satisfied: pymunk<7.0.0,>=6.6.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (6.11.1)\n",
            "Requirement already satisfied: pynput>=1.7.7 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (1.8.1)\n",
            "Requirement already satisfied: pyserial>=3.5 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.5)\n",
            "Requirement already satisfied: pyzmq>=26.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (27.0.0)\n",
            "Requirement already satisfied: rerun-sdk>=0.21.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.14.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=2.4.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: torch>=2.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (2.7.1)\n",
            "Requirement already satisfied: torchcodec>=0.2.1 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.4.0)\n",
            "Requirement already satisfied: torchvision>=0.21.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.22.1)\n",
            "Requirement already satisfied: wandb>=0.16.3 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.20.1)\n",
            "Requirement already satisfied: zarr>=2.17.0 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (3.0.8)\n",
            "Requirement already satisfied: mergedeep~=1.3 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (1.3.4)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: pyyaml-include~=1.4 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: toml~=0.10 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (0.10.2)\n",
            "Requirement already satisfied: typing-inspect~=0.9.0 in /usr/local/lib/python3.11/site-packages (from draccus==0.10.0->lerobot==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/site-packages (from gymnasium==0.29.1->lerobot==0.1.0) (2.2.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/site-packages (from gymnasium==0.29.1->lerobot==0.1.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/site-packages (from gymnasium==0.29.1->lerobot==0.1.0) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/site-packages (from gymnasium==0.29.1->lerobot==0.1.0) (0.0.4)\n",
            "Collecting accelerate>=1.7.0 (from lerobot==0.1.0)\n",
            "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting num2words>=0.5.14 (from lerobot==0.1.0)\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/site-packages (from lerobot==0.1.0) (0.5.3)\n",
            "Collecting transformers>=4.50.3 (from lerobot==0.1.0)\n",
            "  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from accelerate>=1.7.0->lerobot==0.1.0) (7.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/site-packages (from datasets>=2.19.0->lerobot==0.1.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: orderly-set<6,>=5.4.1 in /usr/local/lib/python3.11/site-packages (from deepdiff>=7.0.1->lerobot==0.1.0) (5.4.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/site-packages (from diffusers>=0.27.2->lerobot==0.1.0) (8.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from diffusers>=0.27.2->lerobot==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/site-packages (from diffusers>=0.27.2->lerobot==0.1.0) (11.2.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/site-packages (from flask>=3.0.3->lerobot==0.1.0) (3.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/site-packages (from gdown>=5.1.0->lerobot==0.1.0) (4.13.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.27.1->huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (1.1.5)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.3.4)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/site-packages (from huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.1.9)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (3.0.51)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/site-packages (from imageio[ffmpeg]>=2.34.0->lerobot==0.1.0) (0.6.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/site-packages (from jsonlines>=4.0.0->lerobot==0.1.0) (25.3.0)\n",
            "Collecting docopt>=0.6.2 (from num2words>=0.5.14->lerobot==0.1.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/site-packages (from numba>=0.59.0->lerobot==0.1.0) (0.44.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/site-packages (from omegaconf>=2.3.0->lerobot==0.1.0) (4.9.3)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/site-packages (from pymunk<7.0.0,>=6.6.0->lerobot==0.1.0) (1.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from pynput>=1.7.7->lerobot==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: evdev>=1.3 in /usr/local/lib/python3.11/site-packages (from pynput>=1.7.7->lerobot==0.1.0) (1.9.2)\n",
            "Requirement already satisfied: python-xlib>=0.17 in /usr/local/lib/python3.11/site-packages (from pynput>=1.7.7->lerobot==0.1.0) (0.33)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/site-packages (from torch>=2.2.1->lerobot==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.3.1->torch>=2.2.1->lerobot==0.1.0) (65.6.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.50.3->lerobot==0.1.0)\n",
            "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (6.31.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (2.32.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/site-packages (from wandb>=0.16.3->lerobot==0.1.0) (1.3.6)\n",
            "Requirement already satisfied: donfig>=0.8 in /usr/local/lib/python3.11/site-packages (from zarr>=2.17.0->lerobot==0.1.0) (0.8.1.post1)\n",
            "Requirement already satisfied: numcodecs>=0.14 in /usr/local/lib/python3.11/site-packages (from numcodecs[crc32c]>=0.14->zarr>=2.17.0->lerobot==0.1.0) (0.16.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.17.1->pymunk<7.0.0,>=6.6.0->lerobot==0.1.0) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (3.12.13)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.16.3->lerobot==0.1.0) (4.0.12)\n",
            "Requirement already satisfied: crc32c>=2.7 in /usr/local/lib/python3.11/site-packages (from numcodecs[crc32c]>=0.14->zarr>=2.17.0->lerobot==0.1.0) (2.7.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb>=0.16.3->lerobot==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb>=0.16.3->lerobot==0.1.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3->wandb>=0.16.3->lerobot==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.0->lerobot==0.1.0) (2025.6.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.2.1->lerobot==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/site-packages (from typing-inspect~=0.9.0->draccus==0.10.0->lerobot==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4->gdown>=5.1.0->lerobot==0.1.0) (2.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/site-packages (from importlib_metadata->diffusers>=0.27.2->lerobot==0.1.0) (3.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->lerobot==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->lerobot==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.19.0->lerobot==0.1.0) (2025.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/site-packages (from requests[socks]->gdown>=5.1.0->lerobot==0.1.0) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (6.6.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.19.0->lerobot==0.1.0) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.16.3->lerobot==0.1.0) (5.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface-hub[cli,hf-transfer]>=0.27.1; python_version < \"4.0\"->lerobot==0.1.0) (0.2.13)\n",
            "Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "Downloading transformers-4.53.0-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lerobot, docopt\n",
            "  Building editable for lerobot (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lerobot: filename=lerobot-0.1.0-py3-none-any.whl size=16366 sha256=15dcbc92fc8fe9542afd3f36129de5af8ab5b343281d499ad864d1e0e716556f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8lqg9crn/wheels/15/0d/02/b9c6ff1c78574dee99101ad231194b3425eb4cd784ce8c8338\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=77b6a45120a4350961f6bfb67ce95b732a1098bc1835e1034d9efdf3b57e0974\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built lerobot docopt\n",
            "Installing collected packages: docopt, num2words, tokenizers, transformers, accelerate, lerobot\n",
            "  Attempting uninstall: lerobot\n",
            "    Found existing installation: lerobot 0.1.0\n",
            "    Uninstalling lerobot-0.1.0:\n",
            "      Successfully uninstalled lerobot-0.1.0\n",
            "Successfully installed accelerate-1.8.1 docopt-0.6.2 lerobot-0.1.0 num2words-0.5.14 tokenizers-0.21.2 transformers-4.53.0\n"
          ]
        }
      ],
      "source": [
        "!cd lerobot && pip install -e \".[smolvla]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkzTo4mNwxaC"
      },
      "source": [
        "## Start training SmolVLA with LeRobot\n",
        "\n",
        "This cell runs the `train.py` script from the `lerobot` library to train a robot control policy.  \n",
        "\n",
        "Make sure to adjust the following arguments to your setup:\n",
        "\n",
        "1. `--dataset.repo_id=YOUR_HF_USERNAME/YOUR_DATASET`:  \n",
        "   Replace this with the Hugging Face Hub repo ID where your dataset is stored, e.g., `pepijn223/il_gym0`.\n",
        "\n",
        "2. `--batch_size=64`: means the model processes 64 training samples in parallel before doing one gradient update. Reduce this number if you have a GPU with low memory.\n",
        "\n",
        "3. `--output_dir=outputs/train/...`:  \n",
        "   Directory where training logs and model checkpoints will be saved.\n",
        "\n",
        "4. `--job_name=...`:  \n",
        "   A name for this training job, used for logging and Weights & Biases.\n",
        "\n",
        "5. `--policy.device=cuda`:  \n",
        "   Use `cuda` if training on an NVIDIA GPU. Use `mps` for Apple Silicon, or `cpu` if no GPU is available.\n",
        "\n",
        "6. `--wandb.enable=true`:  \n",
        "   Enables Weights & Biases for visualizing training progress. You must be logged in via `wandb login` before running this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M77792uqA81k",
        "outputId": "9d768130-436c-43f0-cacc-1817a8a92ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json: 2.04kB [00:00, 9.32MB/s]\n",
            "INFO 2025-06-27 12:33:29 ts/train.py:111 {'batch_size': 64,\n",
            " 'dataset': {'episodes': None,\n",
            "             'image_transforms': {'enable': False,\n",
            "                                  'max_num_transforms': 3,\n",
            "                                  'random_order': False,\n",
            "                                  'tfs': {'brightness': {'kwargs': {'brightness': [0.8,\n",
            "                                                                                   1.2]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
            "                                                                               1.2]},\n",
            "                                                       'type': 'ColorJitter',\n",
            "                                                       'weight': 1.0},\n",
            "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
            "                                                                     0.05]},\n",
            "                                                  'type': 'ColorJitter',\n",
            "                                                  'weight': 1.0},\n",
            "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
            "                                                                                   1.5]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
            "                                                                                 1.5]},\n",
            "                                                        'type': 'SharpnessJitter',\n",
            "                                                        'weight': 1.0}}},\n",
            "             'repo_id': 'Damin3927/so101_pickplace',\n",
            "             'revision': None,\n",
            "             'root': None,\n",
            "             'use_imagenet_stats': True,\n",
            "             'video_backend': 'torchcodec'},\n",
            " 'env': None,\n",
            " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
            " 'eval_freq': 20000,\n",
            " 'job_name': 'smolvla_pickplace_4',\n",
            " 'log_freq': 200,\n",
            " 'num_workers': 4,\n",
            " 'optimizer': {'betas': [0.9, 0.95],\n",
            "               'eps': 1e-08,\n",
            "               'grad_clip_norm': 10.0,\n",
            "               'lr': 0.0001,\n",
            "               'type': 'adamw',\n",
            "               'weight_decay': 1e-10},\n",
            " 'output_dir': 'outputs/train/my_smolvla',\n",
            " 'policy': {'adapt_to_pi_aloha': False,\n",
            "            'add_image_special_tokens': False,\n",
            "            'attention_mode': 'cross_attn',\n",
            "            'chunk_size': 50,\n",
            "            'device': 'cuda',\n",
            "            'empty_cameras': 0,\n",
            "            'expert_width_multiplier': 0.75,\n",
            "            'freeze_vision_encoder': True,\n",
            "            'input_features': {'observation.image': {'shape': [3, 256, 256],\n",
            "                                                     'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.image2': {'shape': [3, 256, 256],\n",
            "                                                      'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.image3': {'shape': [3, 256, 256],\n",
            "                                                      'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.state': {'shape': [6],\n",
            "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
            "            'license': None,\n",
            "            'load_vlm_weights': True,\n",
            "            'max_action_dim': 32,\n",
            "            'max_period': 4.0,\n",
            "            'max_state_dim': 32,\n",
            "            'min_period': 0.004,\n",
            "            'n_action_steps': 50,\n",
            "            'n_obs_steps': 1,\n",
            "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
            "            'num_expert_layers': 0,\n",
            "            'num_steps': 10,\n",
            "            'num_vlm_layers': 16,\n",
            "            'optimizer_betas': [0.9, 0.95],\n",
            "            'optimizer_eps': 1e-08,\n",
            "            'optimizer_grad_clip_norm': 10.0,\n",
            "            'optimizer_lr': 0.0001,\n",
            "            'optimizer_weight_decay': 1e-10,\n",
            "            'output_features': {'action': {'shape': [6],\n",
            "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
            "            'pad_language_to': 'max_length',\n",
            "            'prefix_length': 0,\n",
            "            'private': None,\n",
            "            'push_to_hub': True,\n",
            "            'repo_id': 'Damin3927/smolvla_pickplace_ft',\n",
            "            'resize_imgs_with_padding': [512, 512],\n",
            "            'scheduler_decay_lr': 2.5e-06,\n",
            "            'scheduler_decay_steps': 30000,\n",
            "            'scheduler_warmup_steps': 1000,\n",
            "            'self_attn_every_n_layers': 2,\n",
            "            'tags': None,\n",
            "            'tokenizer_max_length': 48,\n",
            "            'train_expert_only': True,\n",
            "            'train_state_proj': True,\n",
            "            'type': 'smolvla',\n",
            "            'use_amp': False,\n",
            "            'use_cache': True,\n",
            "            'use_delta_joint_actions_aloha': False,\n",
            "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
            " 'resume': False,\n",
            " 'save_checkpoint': True,\n",
            " 'save_freq': 2000,\n",
            " 'scheduler': {'decay_lr': 2.5e-06,\n",
            "               'num_decay_steps': 30000,\n",
            "               'num_warmup_steps': 1000,\n",
            "               'peak_lr': 0.0001,\n",
            "               'type': 'cosine_decay_with_warmup'},\n",
            " 'seed': 1000,\n",
            " 'steps': 20000,\n",
            " 'use_policy_training_preset': True,\n",
            " 'wandb': {'disable_artifact': False,\n",
            "           'enable': True,\n",
            "           'entity': 'damin3927-science-tokyo',\n",
            "           'mode': None,\n",
            "           'notes': None,\n",
            "           'project': 'smolvla_pickplace',\n",
            "           'run_id': None}}\n",
            "\u001b[1m\u001b[34mLogs will be synced with wandb.\u001b[0m\n",
            "INFO 2025-06-27 12:33:31 db_utils.py:103 Track this run --> \u001b[1m\u001b[33mhttps://wandb.ai/damin3927-science-tokyo/smolvla_pickplace/runs/w07ce78w\u001b[0m\n",
            "INFO 2025-06-27 12:33:31 ts/train.py:127 Creating dataset\n",
            "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
            "tasks.jsonl: 100% 968/968 [00:00<00:00, 3.37MB/s]\n",
            "\n",
            "episodes_stats.jsonl: 111kB [00:00, 6.80MB/s]\n",
            "\n",
            "info.json: 3.23kB [00:00, 212kB/s]\n",
            "\n",
            "episodes.jsonl: 5.58kB [00:00, 14.9MB/s]\n",
            "Fetching 4 files: 100% 4/4 [00:00<00:00,  8.11it/s]\n",
            "Fetching 159 files:   0% 0/159 [00:00<?, ?it/s]\n",
            "data/chunk-000/episode_000000.parquet:   0% 0.00/24.0k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000001.parquet:   0% 0.00/27.3k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000004.parquet:   0% 0.00/21.9k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000002.parquet:   0% 0.00/24.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000003.parquet:   0% 0.00/24.7k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "README.md: 3.72kB [00:00, 14.6MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 2.46kB [00:00, 9.82MB/s]\n",
            "Fetching 159 files:   1% 1/159 [00:00<00:57,  2.74it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".DS_Store: 100% 6.15k/6.15k [00:00<00:00, 24.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000005.parquet:   0% 0.00/21.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000006.parquet:   0% 0.00/26.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000007.parquet:   0% 0.00/23.1k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000002.parquet: 100% 24.0k/24.0k [00:01<00:00, 21.8kB/s]\n",
            "\n",
            "data/chunk-000/episode_000000.parquet: 100% 24.0k/24.0k [00:01<00:00, 17.7kB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000007.parquet: 100% 23.1k/23.1k [00:00<00:00, 27.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000003.parquet: 100% 24.7k/24.7k [00:01<00:00, 18.4kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000006.parquet: 100% 26.5k/26.5k [00:00<00:00, 26.6kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000005.parquet: 100% 21.2k/21.2k [00:01<00:00, 20.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000000.parquet: 100% 24.0k/24.0k [00:01<00:00, 17.6kB/s]\n",
            "data/chunk-000/episode_000007.parquet: 100% 23.1k/23.1k [00:00<00:00, 27.7kB/s]\n",
            "data/chunk-000/episode_000003.parquet: 100% 24.7k/24.7k [00:01<00:00, 18.4kB/s]\n",
            "data/chunk-000/episode_000006.parquet: 100% 26.5k/26.5k [00:01<00:00, 26.5kB/s]\n",
            "data/chunk-000/episode_000005.parquet: 100% 21.2k/21.2k [00:01<00:00, 20.8kB/s]\n",
            "data/chunk-000/episode_000001.parquet: 100% 27.3k/27.3k [00:01<00:00, 20.0kB/s]\n",
            "Fetching 159 files:   3% 4/159 [00:01<01:01,  2.50it/s]\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000004.parquet: 100% 21.9k/21.9k [00:01<00:00, 15.6kB/s]\n",
            "\n",
            "data/chunk-000/episode_000008.parquet:   0% 0.00/21.2k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000010.parquet:   0% 0.00/35.0k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000009.parquet:   0% 0.00/26.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000014.parquet:   0% 0.00/33.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000013.parquet:   0% 0.00/34.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000011.parquet:   0% 0.00/31.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000015.parquet:   0% 0.00/36.6k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000012.parquet:   0% 0.00/37.5k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "data/chunk-000/episode_000008.parquet: 100% 21.2k/21.2k [00:00<00:00, 41.9kB/s]\n",
            "Fetching 159 files:   8% 12/159 [00:02<00:22,  6.64it/s]\n",
            "data/chunk-000/episode_000016.parquet:   0% 0.00/30.8k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000010.parquet: 100% 35.0k/35.0k [00:01<00:00, 33.5kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000010.parquet: 100% 35.0k/35.0k [00:01<00:00, 33.4kB/s]\n",
            "data/chunk-000/episode_000012.parquet: 100% 37.5k/37.5k [00:00<00:00, 37.7kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000011.parquet: 100% 31.3k/31.3k [00:01<00:00, 27.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000014.parquet: 100% 33.3k/33.3k [00:01<00:00, 29.4kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000015.parquet: 100% 36.6k/36.6k [00:01<00:00, 33.1kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000011.parquet: 100% 31.3k/31.3k [00:01<00:00, 27.8kB/s]\n",
            "data/chunk-000/episode_000014.parquet: 100% 33.3k/33.3k [00:01<00:00, 29.3kB/s]\n",
            "data/chunk-000/episode_000015.parquet: 100% 36.6k/36.6k [00:01<00:00, 33.0kB/s]\n",
            "data/chunk-000/episode_000013.parquet: 100% 34.0k/34.0k [00:01<00:00, 29.9kB/s]\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000009.parquet: 100% 26.0k/26.0k [00:01<00:00, 21.6kB/s]\n",
            "Fetching 159 files:   8% 13/159 [00:03<00:34,  4.26it/s]\n",
            "data/chunk-000/episode_000016.parquet: 100% 30.8k/30.8k [00:00<00:00, 45.8kB/s]\n",
            "\n",
            "data/chunk-000/episode_000018.parquet:   0% 0.00/32.4k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000017.parquet:   0% 0.00/35.9k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000019.parquet:   0% 0.00/23.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000021.parquet:   0% 0.00/26.3k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000023.parquet:   0% 0.00/67.6k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000022.parquet:   0% 0.00/29.4k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000020.parquet:   0% 0.00/28.9k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000024.parquet:   0% 0.00/27.4k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "data/chunk-000/episode_000018.parquet: 100% 32.4k/32.4k [00:00<00:00, 67.7kB/s]\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000019.parquet: 100% 23.0k/23.0k [00:00<00:00, 46.7kB/s]\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000017.parquet: 100% 35.9k/35.9k [00:00<00:00, 59.0kB/s]\n",
            "Fetching 159 files:  13% 21/159 [00:03<00:20,  6.73it/s]\n",
            "data/chunk-000/episode_000025.parquet:   0% 0.00/29.7k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000026.parquet:   0% 0.00/21.1k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000027.parquet:   0% 0.00/22.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000021.parquet: 100% 26.3k/26.3k [00:01<00:00, 26.3kB/s]\n",
            "\n",
            "data/chunk-000/episode_000025.parquet: 100% 29.7k/29.7k [00:00<00:00, 57.9kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000024.parquet: 100% 27.4k/27.4k [00:01<00:00, 26.5kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000023.parquet: 100% 67.6k/67.6k [00:01<00:00, 63.7kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000020.parquet: 100% 28.9k/28.9k [00:01<00:00, 25.4kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000020.parquet: 100% 28.9k/28.9k [00:01<00:00, 25.4kB/s]\n",
            "data/chunk-000/episode_000022.parquet: 100% 29.4k/29.4k [00:01<00:00, 25.7kB/s]\n",
            "Fetching 159 files:  15% 24/159 [00:04<00:22,  5.99it/s]\n",
            "\n",
            "data/chunk-000/episode_000026.parquet: 100% 21.1k/21.1k [00:00<00:00, 41.7kB/s]\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000027.parquet: 100% 22.2k/22.2k [00:00<00:00, 44.3kB/s]\n",
            "\n",
            "data/chunk-000/episode_000028.parquet:   0% 0.00/31.9k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000029.parquet:   0% 0.00/31.1k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000030.parquet:   0% 0.00/59.8k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000031.parquet:   0% 0.00/27.1k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000033.parquet:   0% 0.00/33.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000032.parquet:   0% 0.00/77.9k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000034.parquet:   0% 0.00/31.7k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000035.parquet:   0% 0.00/54.9k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "data/chunk-000/episode_000028.parquet: 100% 31.9k/31.9k [00:00<00:00, 62.4kB/s]\n",
            "Fetching 159 files:  20% 32/159 [00:04<00:15,  8.20it/s]\n",
            "\n",
            "data/chunk-000/episode_000029.parquet: 100% 31.1k/31.1k [00:00<00:00, 63.7kB/s]\n",
            "\n",
            "data/chunk-000/episode_000036.parquet:   0% 0.00/34.6k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000037.parquet:   0% 0.00/41.5k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000030.parquet: 100% 59.8k/59.8k [00:01<00:00, 57.4kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000030.parquet: 100% 59.8k/59.8k [00:01<00:00, 57.3kB/s]\n",
            "data/chunk-000/episode_000034.parquet: 100% 31.7k/31.7k [00:00<00:00, 34.0kB/s]\n",
            "Fetching 159 files:  21% 34/159 [00:05<00:18,  6.81it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000035.parquet: 100% 54.9k/54.9k [00:01<00:00, 54.2kB/s]\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000037.parquet: 100% 41.5k/41.5k [00:00<00:00, 82.2kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000033.parquet: 100% 33.2k/33.2k [00:01<00:00, 29.6kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "data/chunk-000/episode_000036.parquet: 100% 34.6k/34.6k [00:00<00:00, 63.4kB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000032.parquet: 100% 77.9k/77.9k [00:01<00:00, 69.4kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000033.parquet: 100% 33.2k/33.2k [00:01<00:00, 29.5kB/s]\n",
            "data/chunk-000/episode_000036.parquet: 100% 34.6k/34.6k [00:00<00:00, 63.0kB/s]\n",
            "data/chunk-000/episode_000032.parquet: 100% 77.9k/77.9k [00:01<00:00, 69.2kB/s]\n",
            "data/chunk-000/episode_000031.parquet: 100% 27.1k/27.1k [00:01<00:00, 22.1kB/s]\n",
            "Fetching 159 files:  22% 35/159 [00:05<00:18,  6.63it/s]\n",
            "data/chunk-000/episode_000039.parquet:   0% 0.00/60.2k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000038.parquet:   0% 0.00/36.9k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000040.parquet:   0% 0.00/40.7k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000041.parquet:   0% 0.00/44.7k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000045.parquet:   0% 0.00/49.7k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000043.parquet:   0% 0.00/43.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000042.parquet:   0% 0.00/49.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000044.parquet:   0% 0.00/37.9k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "data/chunk-000/episode_000039.parquet: 100% 60.2k/60.2k [00:00<00:00, 120kB/s]\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000038.parquet: 100% 36.9k/36.9k [00:00<00:00, 71.7kB/s]\n",
            "Fetching 159 files:  26% 42/159 [00:06<00:13,  8.66it/s]\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000040.parquet: 100% 40.7k/40.7k [00:00<00:00, 83.6kB/s]\n",
            "\n",
            "data/chunk-000/episode_000046.parquet:   0% 0.00/48.4k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000047.parquet:   0% 0.00/28.2k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000048.parquet:   0% 0.00/34.6k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000045.parquet: 100% 49.7k/49.7k [00:00<00:00, 51.3kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000045.parquet: 100% 49.7k/49.7k [00:00<00:00, 51.2kB/s]\n",
            "data/chunk-000/episode_000041.parquet: 100% 44.7k/44.7k [00:01<00:00, 44.6kB/s]\n",
            "Fetching 159 files:  28% 45/159 [00:06<00:15,  7.26it/s]\n",
            "data/chunk-000/episode_000046.parquet: 100% 48.4k/48.4k [00:00<00:00, 88.9kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000044.parquet: 100% 37.9k/37.9k [00:00<00:00, 40.5kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000043.parquet: 100% 43.0k/43.0k [00:01<00:00, 38.9kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000042.parquet: 100% 49.0k/49.0k [00:01<00:00, 42.3kB/s]\n",
            "Fetching 159 files:  29% 46/159 [00:07<00:16,  6.84it/s]\n",
            "data/chunk-000/episode_000049.parquet:   0% 0.00/43.7k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/19.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/21.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/18.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".DS_Store: 100% 6.15k/6.15k [00:00<00:00, 18.9MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/21.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "data/chunk-000/episode_000047.parquet: 100% 28.2k/28.2k [00:00<00:00, 28.6kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/episode_000047.parquet: 100% 28.2k/28.2k [00:00<00:00, 28.6kB/s]\n",
            "data/chunk-000/episode_000048.parquet: 100% 34.6k/34.6k [00:00<00:00, 37.3kB/s]\n",
            "Fetching 159 files:  32% 51/159 [00:07<00:12,  8.80it/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/16.9M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/15.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/20.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "data/chunk-000/episode_000049.parquet: 100% 43.7k/43.7k [00:00<00:00, 45.0kB/s]\n",
            "Fetching 159 files:  33% 53/159 [00:08<00:16,  6.39it/s]\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/15.6M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 19.3M/19.3M [00:01<00:00, 10.3MB/s]\n",
            "Fetching 159 files:  37% 59/159 [00:09<00:15,  6.36it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 18.7M/18.7M [00:01<00:00, 9.84MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/13.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 21.8M/21.8M [00:02<00:00, 10.5MB/s]\n",
            "Fetching 159 files:  38% 60/159 [00:09<00:16,  6.09it/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 16.9M/16.9M [00:01<00:00, 9.24MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/17.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 15.8M/15.8M [00:01<00:00, 9.05MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/28.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/27.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/20.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 20.8M/20.8M [00:02<00:00, 9.80MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/23.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.top/(…): 100% 15.6M/15.6M [00:01<00:00, 9.00MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/28.6M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 17.7M/17.7M [00:01<00:00, 9.14MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 13.1M/13.1M [00:02<00:00, 6.09MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/28.0M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 20.8M/20.8M [00:01<00:00, 10.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/25.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 28.5M/28.5M [00:02<00:00, 12.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 27.3M/27.3M [00:02<00:00, 12.6MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/27.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/25.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/20.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 23.4M/23.4M [00:02<00:00, 9.46MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 28.6M/28.6M [00:02<00:00, 13.2MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/20.9M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/18.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 25.1M/25.1M [00:01<00:00, 12.7MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 28.0M/28.0M [00:02<00:00, 12.3MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/53.5M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 27.6M/27.6M [00:02<00:00, 12.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 27.6M/27.6M [00:02<00:00, 12.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 25.7M/25.7M [00:02<00:00, 12.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 20.9M/20.9M [00:02<00:00, 10.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/25.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/20.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/15.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 18.5M/18.5M [00:01<00:00, 10.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/14.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.top/(…): 100% 20.9M/20.9M [00:02<00:00, 8.66MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/20.5M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 21.8M/21.8M [00:01<00:00, 11.3MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/19.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 15.8M/15.8M [00:01<00:00, 8.35MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 20.2M/20.2M [00:02<00:00, 9.98MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/58.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 25.6M/25.6M [00:02<00:00, 11.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/20.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 14.8M/14.8M [00:01<00:00, 8.39MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/76.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/28.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 53.5M/53.5M [00:02<00:00, 17.9MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 20.5M/20.5M [00:01<00:00, 10.9MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/22.7M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/63.3M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 19.5M/19.5M [00:01<00:00, 10.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/35.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 20.1M/20.1M [00:01<00:00, 11.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/47.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):  13% 9.63M/76.7M [00:01<00:13, 5.04MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.top/(…): 100% 22.7M/22.7M [00:01<00:00, 11.9MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 28.0M/28.0M [00:02<00:00, 11.9MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/38.5M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/70.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 58.9M/58.9M [00:03<00:00, 19.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/39.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 76.7M/76.7M [00:03<00:00, 23.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/43.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 35.9M/35.9M [00:02<00:00, 15.7MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 63.3M/63.3M [00:03<00:00, 19.3MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/53.1M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   5% 3.42M/70.5M [00:01<00:33, 2.03MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/42.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 47.6M/47.6M [00:02<00:00, 16.9MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/30.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.top/(…): 100% 38.5M/38.5M [00:02<00:00, 15.9MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/45.8M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 39.2M/39.2M [00:02<00:00, 15.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/45.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 43.3M/43.3M [00:02<00:00, 17.9MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 70.5M/70.5M [00:03<00:00, 20.6MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/22.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/38.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 53.1M/53.1M [00:02<00:00, 18.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 53.1M/53.1M [00:02<00:00, 18.1MB/s]\n",
            "videos/chunk-000/observation.images.top/(…): 100% 42.5M/42.5M [00:02<00:00, 17.7MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…):   0% 0.00/45.4M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 30.2M/30.2M [00:02<00:00, 12.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 30.2M/30.2M [00:02<00:00, 12.9MB/s]\n",
            ".DS_Store: 100% 6.15k/6.15k [00:00<00:00, 7.50MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/12.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/15.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.top/(…): 100% 45.8M/45.8M [00:02<00:00, 18.2MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/13.5M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 22.8M/22.8M [00:01<00:00, 11.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 45.7M/45.7M [00:02<00:00, 17.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/15.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/11.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 12.4M/12.4M [00:01<00:00, 7.32MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 15.4M/15.4M [00:01<00:00, 8.66MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 38.1M/38.1M [00:02<00:00, 14.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/10.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/12.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 45.4M/45.4M [00:02<00:00, 18.6MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 13.5M/13.5M [00:01<00:00, 8.06MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/11.1M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/14.1M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 15.4M/15.4M [00:01<00:00, 9.11MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 11.6M/11.6M [00:01<00:00, 7.16MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/30.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/14.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/19.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 10.6M/10.6M [00:01<00:00, 6.69MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/21.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 12.7M/12.7M [00:01<00:00, 7.95MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/19.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.wris(…): 100% 11.1M/11.1M [00:01<00:00, 6.78MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 14.1M/14.1M [00:01<00:00, 8.21MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/23.7M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/31.9M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 14.2M/14.2M [00:01<00:00, 8.77MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/14.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 19.4M/19.4M [00:01<00:00, 10.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/19.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 30.2M/30.2M [00:02<00:00, 13.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/18.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 21.1M/21.1M [00:01<00:00, 11.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/16.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 19.2M/19.2M [00:01<00:00, 10.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/15.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.wris(…): 100% 23.7M/23.7M [00:02<00:00, 11.4MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/17.9M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 19.9M/19.9M [00:01<00:00, 10.6MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 31.9M/31.9M [00:02<00:00, 12.6MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/15.4M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 18.3M/18.3M [00:01<00:00, 9.78MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/46.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/19.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 14.9M/14.9M [00:02<00:00, 5.34MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 16.4M/16.4M [00:01<00:00, 8.84MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/16.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/13.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 15.7M/15.7M [00:02<00:00, 7.86MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/16.5M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.wris(…): 100% 17.9M/17.9M [00:01<00:00, 10.4MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/23.4M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 15.4M/15.4M [00:01<00:00, 8.74MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/18.4M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 19.3M/19.3M [00:01<00:00, 9.84MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 16.2M/16.2M [00:01<00:00, 9.15MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 13.2M/13.2M [00:01<00:00, 7.78MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/39.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/20.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/54.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 46.3M/46.3M [00:02<00:00, 17.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 16.5M/16.5M [00:01<00:00, 9.12MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/19.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/20.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.wris(…): 100% 23.4M/23.4M [00:02<00:00, 10.9MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 18.4M/18.4M [00:01<00:00, 10.0MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/35.3M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/20.5M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 20.4M/20.4M [00:01<00:00, 11.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/27.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 39.8M/39.8M [00:02<00:00, 15.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 19.6M/19.6M [00:01<00:00, 10.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 20.3M/20.3M [00:02<00:00, 10.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/21.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/46.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/30.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 54.1M/54.1M [00:02<00:00, 18.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/30.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 20.5M/20.5M [00:02<00:00, 9.97MB/s]\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/42.7M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "videos/chunk-000/observation.images.wris(…): 100% 35.3M/35.3M [00:02<00:00, 14.9MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/34.5M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 27.1M/27.1M [00:02<00:00, 12.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/27.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 21.6M/21.6M [00:01<00:00, 11.3MB/s]\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/37.6M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 30.7M/30.7M [00:02<00:00, 13.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/34.0M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 46.9M/46.9M [00:02<00:00, 17.6MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 30.1M/30.1M [00:02<00:00, 13.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/17.7M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/28.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 42.7M/42.7M [00:02<00:00, 15.8MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 34.5M/34.5M [00:02<00:00, 13.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 27.1M/27.1M [00:02<00:00, 13.3MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…):   0% 0.00/36.1M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 37.6M/37.6M [00:02<00:00, 15.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 17.7M/17.7M [00:01<00:00, 9.74MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 34.0M/34.0M [00:02<00:00, 14.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 28.2M/28.2M [00:02<00:00, 13.4MB/s]\n",
            "\n",
            "videos/chunk-000/observation.images.wris(…): 100% 36.1M/36.1M [00:02<00:00, 15.0MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "videos/chunk-000/observation.images.top/(…): 100% 21.3M/21.3M [00:46<00:00, 461kB/s]\n",
            "Fetching 159 files: 100% 159/159 [00:53<00:00,  2.97it/s]\n",
            "Resolving data files: 100% 50/50 [00:00<00:00, 143052.66it/s]\n",
            "Downloading data: 100% 50/50 [00:00<00:00, 119427.79files/s]\n",
            "Generating train split: 32368 examples [00:00, 394439.18 examples/s]\n",
            "INFO 2025-06-27 12:34:27 ts/train.py:138 Creating policy\n",
            "processor_config.json: 100% 67.0/67.0 [00:00<00:00, 340kB/s]\n",
            "chat_template.json: 100% 430/430 [00:00<00:00, 1.96MB/s]\n",
            "preprocessor_config.json: 100% 599/599 [00:00<00:00, 2.35MB/s]\n",
            "tokenizer_config.json: 28.6kB [00:00, 63.6MB/s]\n",
            "vocab.json: 801kB [00:00, 7.86MB/s]\n",
            "merges.txt: 466kB [00:00, 12.2MB/s]\n",
            "tokenizer.json: 3.55MB [00:00, 22.4MB/s]\n",
            "added_tokens.json: 4.74kB [00:00, 4.50MB/s]\n",
            "special_tokens_map.json: 100% 868/868 [00:00<00:00, 3.10MB/s]\n",
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n",
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
            "config.json: 3.77kB [00:00, 8.66MB/s]\n",
            "model.safetensors: 100% 2.03G/2.03G [00:09<00:00, 223MB/s]\n",
            "INFO 2025-06-27 12:34:44 modeling.py:991 We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "generation_config.json: 100% 136/136 [00:00<00:00, 585kB/s]\n",
            "Reducing the number of VLM layers to 16 ...\n",
            "model.safetensors: 100% 907M/907M [00:02<00:00, 358MB/s]\n",
            "[standardise_state_dict] 'normalize_inputs.buffer_observation_state.mean'  ←  ['normalize_inputs.so100-red_buffer_observation_state.mean', 'normalize_inputs.so100_buffer_observation_state.mean']\n",
            "[standardise_state_dict] 'normalize_inputs.buffer_observation_state.std'  ←  ['normalize_inputs.so100-red_buffer_observation_state.std', 'normalize_inputs.so100_buffer_observation_state.std']\n",
            "[standardise_state_dict] 'normalize_targets.buffer_action.mean'  ←  ['normalize_targets.so100-red_buffer_action.mean', 'normalize_targets.so100_buffer_action.mean']\n",
            "[standardise_state_dict] 'normalize_targets.buffer_action.std'  ←  ['normalize_targets.so100-red_buffer_action.std', 'normalize_targets.so100_buffer_action.std']\n",
            "[standardise_state_dict] 'unnormalize_outputs.buffer_action.mean'  ←  ['unnormalize_outputs.so100-red_buffer_action.mean', 'unnormalize_outputs.so100_buffer_action.mean']\n",
            "[standardise_state_dict] 'unnormalize_outputs.buffer_action.std'  ←  ['unnormalize_outputs.so100-red_buffer_action.std', 'unnormalize_outputs.so100_buffer_action.std']\n",
            "INFO 2025-06-27 12:34:59 ts/train.py:144 Creating optimizer and scheduler\n",
            "INFO 2025-06-27 12:34:59 ts/train.py:156 \u001b[1m\u001b[33mOutput dir:\u001b[0m outputs/train/my_smolvla\n",
            "INFO 2025-06-27 12:34:59 ts/train.py:159 cfg.steps=20000 (20K)\n",
            "INFO 2025-06-27 12:34:59 ts/train.py:160 dataset.num_frames=32368 (32K)\n",
            "INFO 2025-06-27 12:34:59 ts/train.py:161 dataset.num_episodes=50\n",
            "INFO 2025-06-27 12:34:59 ts/train.py:162 num_learnable_params=99880992 (100M)\n",
            "INFO 2025-06-27 12:34:59 ts/train.py:163 num_total_params=450046212 (450M)\n",
            "INFO 2025-06-27 12:34:59 ts/train.py:202 Start offline training on a fixed dataset\n",
            "INFO 2025-06-27 12:43:58 ts/train.py:232 step:200 smpl:13K ep:20 epch:0.40 loss:0.089 grdn:0.569 lr:1.0e-05 updt_s:0.873 data_s:1.819\n",
            "WARNING 2025-06-27 12:43:58 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 12:43:58 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 12:52:44 ts/train.py:232 step:400 smpl:26K ep:40 epch:0.79 loss:0.054 grdn:0.369 lr:3.0e-05 updt_s:0.807 data_s:1.822\n",
            "WARNING 2025-06-27 12:52:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 12:52:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 13:01:02 ts/train.py:232 step:600 smpl:38K ep:59 epch:1.19 loss:0.050 grdn:0.404 lr:5.0e-05 updt_s:0.813 data_s:1.673\n",
            "WARNING 2025-06-27 13:01:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 13:01:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 13:08:43 ts/train.py:232 step:800 smpl:51K ep:79 epch:1.58 loss:0.047 grdn:0.420 lr:7.0e-05 updt_s:0.808 data_s:1.495\n",
            "WARNING 2025-06-27 13:08:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 13:08:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 13:16:25 ts/train.py:232 step:1K smpl:64K ep:99 epch:1.98 loss:0.045 grdn:0.408 lr:9.0e-05 updt_s:0.813 data_s:1.498\n",
            "WARNING 2025-06-27 13:16:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 13:16:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 13:25:36 ts/train.py:232 step:1K smpl:77K ep:119 epch:2.37 loss:0.042 grdn:0.405 lr:1.0e-04 updt_s:1.129 data_s:1.620\n",
            "WARNING 2025-06-27 13:25:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 13:25:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 13:34:32 ts/train.py:232 step:1K smpl:90K ep:138 epch:2.77 loss:0.039 grdn:0.371 lr:1.0e-04 updt_s:0.807 data_s:1.871\n",
            "WARNING 2025-06-27 13:34:32 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 13:34:32 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 13:43:13 ts/train.py:232 step:2K smpl:102K ep:158 epch:3.16 loss:0.036 grdn:0.357 lr:9.9e-05 updt_s:0.807 data_s:1.793\n",
            "WARNING 2025-06-27 13:43:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 13:43:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 13:50:51 ts/train.py:232 step:2K smpl:115K ep:178 epch:3.56 loss:0.034 grdn:0.349 lr:9.9e-05 updt_s:0.809 data_s:1.480\n",
            "WARNING 2025-06-27 13:50:51 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 13:50:51 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 13:58:43 ts/train.py:232 step:2K smpl:128K ep:198 epch:3.95 loss:0.031 grdn:0.316 lr:9.9e-05 updt_s:0.809 data_s:1.549\n",
            "WARNING 2025-06-27 13:58:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 13:58:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 13:58:43 ts/train.py:241 Checkpoint policy after step 2000\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 14:06:34 ts/train.py:232 step:2K smpl:141K ep:217 epch:4.35 loss:0.029 grdn:0.309 lr:9.9e-05 updt_s:0.816 data_s:1.512\n",
            "WARNING 2025-06-27 14:06:34 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 14:06:34 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 14:14:16 ts/train.py:232 step:2K smpl:154K ep:237 epch:4.75 loss:0.028 grdn:0.305 lr:9.9e-05 updt_s:0.810 data_s:1.494\n",
            "WARNING 2025-06-27 14:14:16 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 14:14:16 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 14:22:03 ts/train.py:232 step:3K smpl:166K ep:257 epch:5.14 loss:0.027 grdn:0.296 lr:9.8e-05 updt_s:0.808 data_s:1.526\n",
            "WARNING 2025-06-27 14:22:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 14:22:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 14:29:46 ts/train.py:232 step:3K smpl:179K ep:277 epch:5.54 loss:0.025 grdn:0.278 lr:9.8e-05 updt_s:0.809 data_s:1.504\n",
            "WARNING 2025-06-27 14:29:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 14:29:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 14:37:42 ts/train.py:232 step:3K smpl:192K ep:297 epch:5.93 loss:0.024 grdn:0.285 lr:9.8e-05 updt_s:0.807 data_s:1.569\n",
            "WARNING 2025-06-27 14:37:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 14:37:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 14:45:27 ts/train.py:232 step:3K smpl:205K ep:316 epch:6.33 loss:0.023 grdn:0.275 lr:9.7e-05 updt_s:0.807 data_s:1.512\n",
            "WARNING 2025-06-27 14:45:27 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 14:45:27 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 14:53:06 ts/train.py:232 step:3K smpl:218K ep:336 epch:6.72 loss:0.022 grdn:0.253 lr:9.7e-05 updt_s:0.814 data_s:1.482\n",
            "WARNING 2025-06-27 14:53:06 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 14:53:06 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 15:01:17 ts/train.py:232 step:4K smpl:230K ep:356 epch:7.12 loss:0.021 grdn:0.253 lr:9.7e-05 updt_s:1.011 data_s:1.439\n",
            "WARNING 2025-06-27 15:01:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 15:01:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 15:09:01 ts/train.py:232 step:4K smpl:243K ep:376 epch:7.51 loss:0.021 grdn:0.262 lr:9.6e-05 updt_s:0.808 data_s:1.506\n",
            "WARNING 2025-06-27 15:09:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 15:09:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 15:16:35 ts/train.py:232 step:4K smpl:256K ep:395 epch:7.91 loss:0.020 grdn:0.250 lr:9.6e-05 updt_s:0.809 data_s:1.463\n",
            "WARNING 2025-06-27 15:16:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 15:16:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 15:16:35 ts/train.py:241 Checkpoint policy after step 4000\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 15:24:41 ts/train.py:232 step:4K smpl:269K ep:415 epch:8.30 loss:0.019 grdn:0.248 lr:9.6e-05 updt_s:0.934 data_s:1.451\n",
            "WARNING 2025-06-27 15:24:41 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 15:24:41 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 15:32:19 ts/train.py:232 step:4K smpl:282K ep:435 epch:8.70 loss:0.019 grdn:0.239 lr:9.5e-05 updt_s:0.858 data_s:1.430\n",
            "WARNING 2025-06-27 15:32:19 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 15:32:19 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 15:40:36 ts/train.py:232 step:5K smpl:294K ep:455 epch:9.10 loss:0.018 grdn:0.241 lr:9.5e-05 updt_s:0.969 data_s:1.509\n",
            "WARNING 2025-06-27 15:40:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 15:40:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 15:48:21 ts/train.py:232 step:5K smpl:307K ep:475 epch:9.49 loss:0.018 grdn:0.234 lr:9.4e-05 updt_s:0.896 data_s:1.426\n",
            "WARNING 2025-06-27 15:48:21 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 15:48:21 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 15:55:59 ts/train.py:232 step:5K smpl:320K ep:494 epch:9.89 loss:0.017 grdn:0.230 lr:9.4e-05 updt_s:0.813 data_s:1.474\n",
            "WARNING 2025-06-27 15:55:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 15:55:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 16:04:09 ts/train.py:232 step:5K smpl:333K ep:514 epch:10.28 loss:0.016 grdn:0.211 lr:9.3e-05 updt_s:0.987 data_s:1.458\n",
            "WARNING 2025-06-27 16:04:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 16:04:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 16:12:14 ts/train.py:232 step:5K smpl:346K ep:534 epch:10.68 loss:0.016 grdn:0.212 lr:9.3e-05 updt_s:0.950 data_s:1.475\n",
            "WARNING 2025-06-27 16:12:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 16:12:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 16:20:29 ts/train.py:232 step:6K smpl:358K ep:554 epch:11.07 loss:0.016 grdn:0.219 lr:9.2e-05 updt_s:0.984 data_s:1.485\n",
            "WARNING 2025-06-27 16:20:29 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 16:20:29 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 16:28:11 ts/train.py:232 step:6K smpl:371K ep:573 epch:11.47 loss:0.015 grdn:0.215 lr:9.2e-05 updt_s:0.890 data_s:1.415\n",
            "WARNING 2025-06-27 16:28:11 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 16:28:11 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 16:35:50 ts/train.py:232 step:6K smpl:384K ep:593 epch:11.86 loss:0.016 grdn:0.216 lr:9.1e-05 updt_s:0.868 data_s:1.427\n",
            "WARNING 2025-06-27 16:35:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 16:35:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 16:35:50 ts/train.py:241 Checkpoint policy after step 6000\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 16:43:59 ts/train.py:232 step:6K smpl:397K ep:613 epch:12.26 loss:0.015 grdn:0.209 lr:9.0e-05 updt_s:1.082 data_s:1.311\n",
            "WARNING 2025-06-27 16:43:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 16:43:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 16:51:53 ts/train.py:232 step:6K smpl:410K ep:633 epch:12.65 loss:0.015 grdn:0.207 lr:9.0e-05 updt_s:1.010 data_s:1.360\n",
            "WARNING 2025-06-27 16:51:53 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 16:51:53 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 16:59:57 ts/train.py:232 step:7K smpl:422K ep:652 epch:13.05 loss:0.014 grdn:0.206 lr:8.9e-05 updt_s:0.831 data_s:1.582\n",
            "WARNING 2025-06-27 16:59:57 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 16:59:57 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 17:07:50 ts/train.py:232 step:7K smpl:435K ep:672 epch:13.45 loss:0.014 grdn:0.194 lr:8.8e-05 updt_s:0.827 data_s:1.540\n",
            "WARNING 2025-06-27 17:07:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 17:07:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 17:16:00 ts/train.py:232 step:7K smpl:448K ep:692 epch:13.84 loss:0.014 grdn:0.199 lr:8.8e-05 updt_s:0.808 data_s:1.635\n",
            "WARNING 2025-06-27 17:16:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 17:16:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 17:23:40 ts/train.py:232 step:7K smpl:461K ep:712 epch:14.24 loss:0.013 grdn:0.198 lr:8.7e-05 updt_s:0.810 data_s:1.490\n",
            "WARNING 2025-06-27 17:23:40 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 17:23:40 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 17:30:54 ts/train.py:232 step:7K smpl:474K ep:732 epch:14.63 loss:0.013 grdn:0.190 lr:8.6e-05 updt_s:0.811 data_s:1.357\n",
            "WARNING 2025-06-27 17:30:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 17:30:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 17:38:21 ts/train.py:232 step:8K smpl:486K ep:751 epch:15.03 loss:0.013 grdn:0.195 lr:8.6e-05 updt_s:0.838 data_s:1.393\n",
            "WARNING 2025-06-27 17:38:21 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 17:38:21 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 17:46:09 ts/train.py:232 step:8K smpl:499K ep:771 epch:15.42 loss:0.012 grdn:0.184 lr:8.5e-05 updt_s:0.983 data_s:1.354\n",
            "WARNING 2025-06-27 17:46:09 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 17:46:09 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 17:54:06 ts/train.py:232 step:8K smpl:512K ep:791 epch:15.82 loss:0.013 grdn:0.194 lr:8.4e-05 updt_s:0.958 data_s:1.420\n",
            "WARNING 2025-06-27 17:54:06 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 17:54:06 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 17:54:06 ts/train.py:241 Checkpoint policy after step 8000\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 18:02:03 ts/train.py:232 step:8K smpl:525K ep:811 epch:16.21 loss:0.012 grdn:0.178 lr:8.3e-05 updt_s:1.075 data_s:1.262\n",
            "WARNING 2025-06-27 18:02:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 18:02:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 18:09:52 ts/train.py:232 step:8K smpl:538K ep:830 epch:16.61 loss:0.012 grdn:0.185 lr:8.3e-05 updt_s:0.912 data_s:1.428\n",
            "WARNING 2025-06-27 18:09:52 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 18:09:52 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 18:17:48 ts/train.py:232 step:9K smpl:550K ep:850 epch:17.00 loss:0.012 grdn:0.184 lr:8.2e-05 updt_s:0.856 data_s:1.519\n",
            "WARNING 2025-06-27 18:17:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 18:17:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 18:25:44 ts/train.py:232 step:9K smpl:563K ep:870 epch:17.40 loss:0.011 grdn:0.180 lr:8.1e-05 updt_s:0.891 data_s:1.488\n",
            "WARNING 2025-06-27 18:25:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 18:25:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 18:33:38 ts/train.py:232 step:9K smpl:576K ep:890 epch:17.80 loss:0.011 grdn:0.180 lr:8.0e-05 updt_s:0.953 data_s:1.414\n",
            "WARNING 2025-06-27 18:33:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 18:33:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 18:41:57 ts/train.py:232 step:9K smpl:589K ep:910 epch:18.19 loss:0.011 grdn:0.177 lr:7.9e-05 updt_s:1.075 data_s:1.414\n",
            "WARNING 2025-06-27 18:41:57 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 18:41:57 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 18:49:48 ts/train.py:232 step:9K smpl:602K ep:929 epch:18.59 loss:0.011 grdn:0.170 lr:7.9e-05 updt_s:1.069 data_s:1.282\n",
            "WARNING 2025-06-27 18:49:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 18:49:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 18:57:41 ts/train.py:232 step:10K smpl:614K ep:949 epch:18.98 loss:0.011 grdn:0.175 lr:7.8e-05 updt_s:0.889 data_s:1.473\n",
            "WARNING 2025-06-27 18:57:41 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 18:57:41 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 19:05:37 ts/train.py:232 step:10K smpl:627K ep:969 epch:19.38 loss:0.010 grdn:0.168 lr:7.7e-05 updt_s:0.829 data_s:1.549\n",
            "WARNING 2025-06-27 19:05:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 19:05:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 19:13:14 ts/train.py:232 step:10K smpl:640K ep:989 epch:19.77 loss:0.010 grdn:0.165 lr:7.6e-05 updt_s:0.811 data_s:1.472\n",
            "WARNING 2025-06-27 19:13:14 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 19:13:14 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 19:13:14 ts/train.py:241 Checkpoint policy after step 10000\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 19:21:17 ts/train.py:232 step:10K smpl:653K ep:1K epch:20.17 loss:0.010 grdn:0.160 lr:7.5e-05 updt_s:0.956 data_s:1.403\n",
            "WARNING 2025-06-27 19:21:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 19:21:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 19:28:59 ts/train.py:232 step:10K smpl:666K ep:1K epch:20.56 loss:0.010 grdn:0.166 lr:7.4e-05 updt_s:0.808 data_s:1.501\n",
            "WARNING 2025-06-27 19:28:59 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 19:28:59 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 19:36:36 ts/train.py:232 step:11K smpl:678K ep:1K epch:20.96 loss:0.010 grdn:0.165 lr:7.3e-05 updt_s:0.808 data_s:1.471\n",
            "WARNING 2025-06-27 19:36:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 19:36:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 19:44:43 ts/train.py:232 step:11K smpl:691K ep:1K epch:21.35 loss:0.010 grdn:0.162 lr:7.2e-05 updt_s:0.806 data_s:1.626\n",
            "WARNING 2025-06-27 19:44:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 19:44:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 19:52:43 ts/train.py:232 step:11K smpl:704K ep:1K epch:21.75 loss:0.010 grdn:0.160 lr:7.2e-05 updt_s:0.809 data_s:1.590\n",
            "WARNING 2025-06-27 19:52:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 19:52:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 20:00:34 ts/train.py:232 step:11K smpl:717K ep:1K epch:22.15 loss:0.009 grdn:0.154 lr:7.1e-05 updt_s:0.807 data_s:1.546\n",
            "WARNING 2025-06-27 20:00:34 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 20:00:34 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 20:08:12 ts/train.py:232 step:11K smpl:730K ep:1K epch:22.54 loss:0.009 grdn:0.150 lr:7.0e-05 updt_s:0.809 data_s:1.476\n",
            "WARNING 2025-06-27 20:08:12 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 20:08:12 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 20:15:48 ts/train.py:232 step:12K smpl:742K ep:1K epch:22.94 loss:0.009 grdn:0.152 lr:6.9e-05 updt_s:0.810 data_s:1.470\n",
            "WARNING 2025-06-27 20:15:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 20:15:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 20:23:25 ts/train.py:232 step:12K smpl:755K ep:1K epch:23.33 loss:0.009 grdn:0.155 lr:6.8e-05 updt_s:0.817 data_s:1.463\n",
            "WARNING 2025-06-27 20:23:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 20:23:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 20:31:02 ts/train.py:232 step:12K smpl:768K ep:1K epch:23.73 loss:0.008 grdn:0.150 lr:6.7e-05 updt_s:0.809 data_s:1.473\n",
            "WARNING 2025-06-27 20:31:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 20:31:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 20:31:02 ts/train.py:241 Checkpoint policy after step 12000\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 20:38:54 ts/train.py:232 step:12K smpl:781K ep:1K epch:24.12 loss:0.008 grdn:0.152 lr:6.6e-05 updt_s:1.008 data_s:1.319\n",
            "WARNING 2025-06-27 20:38:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 20:38:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 20:46:44 ts/train.py:232 step:12K smpl:794K ep:1K epch:24.52 loss:0.008 grdn:0.146 lr:6.5e-05 updt_s:0.809 data_s:1.542\n",
            "WARNING 2025-06-27 20:46:44 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 20:46:44 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 20:54:30 ts/train.py:232 step:13K smpl:806K ep:1K epch:24.91 loss:0.008 grdn:0.145 lr:6.4e-05 updt_s:0.812 data_s:1.515\n",
            "WARNING 2025-06-27 20:54:30 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 20:54:30 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 21:02:28 ts/train.py:232 step:13K smpl:819K ep:1K epch:25.31 loss:0.008 grdn:0.141 lr:6.3e-05 updt_s:0.883 data_s:1.500\n",
            "WARNING 2025-06-27 21:02:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 21:02:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 21:10:18 ts/train.py:232 step:13K smpl:832K ep:1K epch:25.70 loss:0.008 grdn:0.144 lr:6.2e-05 updt_s:0.848 data_s:1.503\n",
            "WARNING 2025-06-27 21:10:18 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 21:10:18 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 21:18:13 ts/train.py:232 step:13K smpl:845K ep:1K epch:26.10 loss:0.008 grdn:0.143 lr:6.1e-05 updt_s:0.890 data_s:1.478\n",
            "WARNING 2025-06-27 21:18:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 21:18:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 21:26:08 ts/train.py:232 step:13K smpl:858K ep:1K epch:26.50 loss:0.008 grdn:0.139 lr:6.0e-05 updt_s:0.883 data_s:1.488\n",
            "WARNING 2025-06-27 21:26:08 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 21:26:08 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 21:34:00 ts/train.py:232 step:14K smpl:870K ep:1K epch:26.89 loss:0.007 grdn:0.133 lr:5.9e-05 updt_s:0.844 data_s:1.515\n",
            "WARNING 2025-06-27 21:34:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 21:34:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 21:42:13 ts/train.py:232 step:14K smpl:883K ep:1K epch:27.29 loss:0.007 grdn:0.131 lr:5.8e-05 updt_s:1.019 data_s:1.441\n",
            "WARNING 2025-06-27 21:42:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 21:42:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 21:50:02 ts/train.py:232 step:14K smpl:896K ep:1K epch:27.68 loss:0.008 grdn:0.134 lr:5.7e-05 updt_s:0.923 data_s:1.419\n",
            "WARNING 2025-06-27 21:50:02 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 21:50:02 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 21:50:02 ts/train.py:241 Checkpoint policy after step 14000\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 21:58:00 ts/train.py:232 step:14K smpl:909K ep:1K epch:28.08 loss:0.007 grdn:0.127 lr:5.6e-05 updt_s:0.934 data_s:1.407\n",
            "WARNING 2025-06-27 21:58:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 21:58:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 22:05:49 ts/train.py:232 step:14K smpl:922K ep:1K epch:28.47 loss:0.007 grdn:0.125 lr:5.5e-05 updt_s:0.910 data_s:1.434\n",
            "WARNING 2025-06-27 22:05:49 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 22:05:49 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 22:13:37 ts/train.py:232 step:15K smpl:934K ep:1K epch:28.87 loss:0.007 grdn:0.129 lr:5.4e-05 updt_s:0.950 data_s:1.381\n",
            "WARNING 2025-06-27 22:13:37 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 22:13:37 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 22:21:43 ts/train.py:232 step:15K smpl:947K ep:1K epch:29.26 loss:0.007 grdn:0.130 lr:5.3e-05 updt_s:1.044 data_s:1.383\n",
            "WARNING 2025-06-27 22:21:43 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 22:21:43 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 22:29:36 ts/train.py:232 step:15K smpl:960K ep:1K epch:29.66 loss:0.007 grdn:0.125 lr:5.2e-05 updt_s:0.962 data_s:1.402\n",
            "WARNING 2025-06-27 22:29:36 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 22:29:36 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 22:37:45 ts/train.py:232 step:15K smpl:973K ep:2K epch:30.05 loss:0.006 grdn:0.123 lr:5.1e-05 updt_s:0.950 data_s:1.487\n",
            "WARNING 2025-06-27 22:37:45 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 22:37:45 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 22:45:51 ts/train.py:232 step:15K smpl:986K ep:2K epch:30.45 loss:0.006 grdn:0.122 lr:5.0e-05 updt_s:1.016 data_s:1.408\n",
            "WARNING 2025-06-27 22:45:51 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 22:45:51 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 22:54:00 ts/train.py:232 step:16K smpl:998K ep:2K epch:30.85 loss:0.007 grdn:0.123 lr:4.9e-05 updt_s:1.144 data_s:1.297\n",
            "WARNING 2025-06-27 22:54:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 22:54:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 23:01:28 ts/train.py:232 step:16K smpl:1M ep:2K epch:31.24 loss:0.006 grdn:0.120 lr:4.8e-05 updt_s:0.912 data_s:1.325\n",
            "WARNING 2025-06-27 23:01:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 23:01:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 23:08:31 ts/train.py:232 step:16K smpl:1M ep:2K epch:31.64 loss:0.006 grdn:0.123 lr:4.7e-05 updt_s:0.812 data_s:1.301\n",
            "WARNING 2025-06-27 23:08:31 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 23:08:31 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 23:08:31 ts/train.py:241 Checkpoint policy after step 16000\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 23:15:50 ts/train.py:232 step:16K smpl:1M ep:2K epch:32.03 loss:0.006 grdn:0.113 lr:4.6e-05 updt_s:0.931 data_s:1.203\n",
            "WARNING 2025-06-27 23:15:50 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 23:15:50 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 23:24:00 ts/train.py:232 step:16K smpl:1M ep:2K epch:32.43 loss:0.006 grdn:0.112 lr:4.5e-05 updt_s:0.990 data_s:1.457\n",
            "WARNING 2025-06-27 23:24:00 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 23:24:00 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 23:32:13 ts/train.py:232 step:17K smpl:1M ep:2K epch:32.82 loss:0.006 grdn:0.115 lr:4.4e-05 updt_s:1.009 data_s:1.451\n",
            "WARNING 2025-06-27 23:32:13 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 23:32:13 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-27 23:40:17 ts/train.py:232 step:17K smpl:1M ep:2K epch:33.22 loss:0.006 grdn:0.114 lr:4.3e-05 updt_s:0.959 data_s:1.456\n",
            "WARNING 2025-06-27 23:40:17 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 23:40:17 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 23:48:03 ts/train.py:232 step:17K smpl:1M ep:2K epch:33.61 loss:0.006 grdn:0.114 lr:4.2e-05 updt_s:0.977 data_s:1.351\n",
            "WARNING 2025-06-27 23:48:03 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 23:48:03 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-27 23:56:01 ts/train.py:232 step:17K smpl:1M ep:2K epch:34.01 loss:0.006 grdn:0.118 lr:4.1e-05 updt_s:0.942 data_s:1.440\n",
            "WARNING 2025-06-27 23:56:01 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-27 23:56:01 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-28 00:04:28 ts/train.py:232 step:17K smpl:1M ep:2K epch:34.40 loss:0.006 grdn:0.108 lr:4.0e-05 updt_s:1.243 data_s:1.289\n",
            "WARNING 2025-06-28 00:04:28 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 00:04:28 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 00:12:38 ts/train.py:232 step:18K smpl:1M ep:2K epch:34.80 loss:0.006 grdn:0.111 lr:3.9e-05 updt_s:1.272 data_s:1.172\n",
            "WARNING 2025-06-28 00:12:38 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 00:12:38 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-28 00:20:46 ts/train.py:232 step:18K smpl:1M ep:2K epch:35.20 loss:0.006 grdn:0.107 lr:3.8e-05 updt_s:1.212 data_s:1.222\n",
            "WARNING 2025-06-28 00:20:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 00:20:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 00:28:48 ts/train.py:232 step:18K smpl:1M ep:2K epch:35.59 loss:0.005 grdn:0.107 lr:3.7e-05 updt_s:1.190 data_s:1.209\n",
            "WARNING 2025-06-28 00:28:48 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 00:28:48 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 00:28:48 ts/train.py:241 Checkpoint policy after step 18000\n",
            "INFO 2025-06-28 00:36:42 ts/train.py:232 step:18K smpl:1M ep:2K epch:35.99 loss:0.006 grdn:0.109 lr:3.6e-05 updt_s:0.942 data_s:1.370\n",
            "WARNING 2025-06-28 00:36:42 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 00:36:42 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-28 00:45:05 ts/train.py:232 step:18K smpl:1M ep:2K epch:36.38 loss:0.005 grdn:0.108 lr:3.5e-05 updt_s:1.147 data_s:1.366\n",
            "WARNING 2025-06-28 00:45:05 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 00:45:05 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 00:53:15 ts/train.py:232 step:19K smpl:1M ep:2K epch:36.78 loss:0.005 grdn:0.103 lr:3.4e-05 updt_s:1.226 data_s:1.214\n",
            "WARNING 2025-06-28 00:53:15 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 00:53:15 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-28 01:01:25 ts/train.py:232 step:19K smpl:1M ep:2K epch:37.17 loss:0.005 grdn:0.100 lr:3.3e-05 updt_s:1.176 data_s:1.268\n",
            "WARNING 2025-06-28 01:01:25 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 01:01:25 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 01:09:35 ts/train.py:232 step:19K smpl:1M ep:2K epch:37.57 loss:0.005 grdn:0.104 lr:3.2e-05 updt_s:1.073 data_s:1.372\n",
            "WARNING 2025-06-28 01:09:35 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 01:09:35 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 01:17:46 ts/train.py:232 step:19K smpl:1M ep:2K epch:37.96 loss:0.005 grdn:0.101 lr:3.1e-05 updt_s:1.064 data_s:1.389\n",
            "WARNING 2025-06-28 01:17:46 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 01:17:46 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-28 01:25:54 ts/train.py:232 step:19K smpl:1M ep:2K epch:38.36 loss:0.005 grdn:0.097 lr:3.0e-05 updt_s:1.101 data_s:1.331\n",
            "WARNING 2025-06-28 01:25:54 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 01:25:54 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 01:33:55 ts/train.py:232 step:20K smpl:1M ep:2K epch:38.75 loss:0.005 grdn:0.100 lr:2.9e-05 updt_s:1.111 data_s:1.290\n",
            "WARNING 2025-06-28 01:33:55 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 01:33:55 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "INFO 2025-06-28 01:42:07 ts/train.py:232 step:20K smpl:1M ep:2K epch:39.15 loss:0.005 grdn:0.096 lr:2.8e-05 updt_s:1.143 data_s:1.312\n",
            "WARNING 2025-06-28 01:42:07 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 01:42:07 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 01:50:10 ts/train.py:232 step:20K smpl:1M ep:2K epch:39.55 loss:0.005 grdn:0.098 lr:2.7e-05 updt_s:1.259 data_s:1.147\n",
            "WARNING 2025-06-28 01:50:10 db_utils.py:141 WandB logging of key \"losses_after_forward\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "WARNING 2025-06-28 01:50:10 db_utils.py:141 WandB logging of key \"losses_after_rm_padding\" was ignored as its type \"<class 'torch.Tensor'>\" is not handled by this wrapper.\n",
            "INFO 2025-06-28 01:50:10 ts/train.py:241 Checkpoint policy after step 20000\n",
            "INFO 2025-06-28 01:50:21 ts/train.py:283 End of training\n",
            "Uploading...: 100% 907M/907M [00:10<00:00, 82.7MB/s]\n",
            "INFO 2025-06-28 01:50:41 etrained.py:223 Model pushed to https://huggingface.co/Damin3927/smolvla_pickplace_ft\n"
          ]
        }
      ],
      "source": [
        "!cd lerobot && python lerobot/scripts/train.py \\\n",
        "  --policy.path=lerobot/smolvla_base \\\n",
        "  --policy.repo_id=Damin3927/smolvla_pickplace_ft \\\n",
        "  --dataset.repo_id=Damin3927/so101_pickplace \\\n",
        "  --batch_size=64 \\\n",
        "  --steps=20000 \\\n",
        "  --save_freq=2000 \\\n",
        "  --output_dir=outputs/train/my_smolvla \\\n",
        "  --job_name=smolvla_pickplace_4 \\\n",
        "  --policy.device=cuda \\\n",
        "  --wandb.enable=true \\\n",
        "  --wandb.entity=damin3927-science-tokyo \\\n",
        "  --wandb.project=smolvla_pickplace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FeT7lwRA81k"
      },
      "source": [
        "## Login into Hugging Face Hub\n",
        "Now after training is done login into the Hugging Face hub and upload the last checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFMLGuVkH7UN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0828ce5f-20de-4d4a-9eeb-8e568e71cedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start hashing 3 files.\n",
            "Finished hashing 3 files.\n",
            "Uploading...: 100% 907M/907M [00:05<00:00, 174MB/s]\n",
            "https://huggingface.co/Damin3927/so101_pickplace/tree/main/.\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli upload Damin3927/so101_pickplace \\\n",
        "  /content/lerobot/outputs/train/my_smolvla/checkpoints/last/pretrained_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "NlcPBa65qVaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhpOShOQYJt0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}